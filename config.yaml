project_name: face_detection # for wandb
exp_name: deimv2 # experiment name

exp: ${exp_name}_${now_dir}

model_name: l # model size (n, s, m, l, x)

train:
  ### Paths ###
  root: dataset/face-detection # project root with dataset and outputs
  pretrained_dataset: coco # coco, obj2coco
  
  # train from scratch -> null
  #pretrained_model_path: pretrained/dfine_${model_name}_${train.pretrained_dataset}.pth # dfine_m_obj2coco.pth
  pretrained_model_path: null

  data_path: ${train.root}/dataset # path to dataset
  path_to_test_data: ${train.root}/test/images  # path to test set, used in infer script
  path_to_save: ${train.root}/output/models/${exp} # where to save output

  debug_img_path: ${train.root}/output/debug_images
  eval_preds_path: ${train.root}/output/eval_preds
  bench_img_path: ${train.root}/output/bench_imgs
  infer_path: ${train.root}/output/infer

  ### Configs ###
  use_wandb: False
  device: cuda
  label_to_name: # dataset's classes
    0: face

  use_one_class: True

  ### DEIMv2 Specific Configs ### (MOVED HERE)
  HybridEncoder:
    version: deim
    fuse_op: sum
    csp_type: csp2

  DEIMTransformer:
    activation: silu
    mlp_act: silu
    # base_size_repeat: 4
    use_gateway: True
    share_bbox_head: False
    share_score_head: False
  
  DEIMCriterion:
    weight_dict: {loss_mal: 1, loss_bbox: 5, loss_giou: 2, loss_fgl: 0.15, loss_ddf: 1.5}
    losses: ['mal', 'boxes', 'local']
    gamma: 1.5
    alpha: 0.75
    reg_max: 32
    use_uni_set: True # Use a union of matched indices for box/local losses
    matcher:
      change_matcher: True
      iou_order_alpha: 4.0
      matcher_change_epoch: 45 # Epoch to switch matcher behavior

  ### DINOv3STAs default config
  DINOv3STAs:
    name: dinov3_vits16
    weights_path: ./ckpts/dinov3_vits16_pretrain_lvd1689m-08c60483.pth
    interaction_indexes: [5,8,11]
    finetune: True
    conv_inplane: 32
    hidden_dim: 224


  ### Dataloader ###
  dataloader:
    base_size_repeat: 4 # configures multiscale training behaviour

  img_size: [640, 640] # (h, w)
  keep_ratio: False # image aspect ratio, if True - image will be padded
  to_visualize_eval: True # save images with gt and preds
  debug_img_processing: True # save images after preprocessing

  amp_enabled: True # use automatic mixed precision
  clip_max_norm: 0.1 # gradient clipping

  batch_size: 8 # physical, should fit on the device
  b_accum_steps: 1 # grad accumulation (n * bs)
  epochs: 58 # Total epochs
  early_stopping: 10 # 0 - no early stopping
  ignore_background_epochs: 0 # background images are not used for N epochs in train set
  num_workers: 12

  ### Validation ###
  conf_thresh: 0.5
  iou_thresh: 0.5

  ### EMA ###
  use_ema: True # use exponential moving average model
  ema_momentum: 0.9999

  ### Optimizer ###
  optimizer:
    type: AdamW
    lr: ${train.lrs.${model_name}.base_lr}
    betas: [0.9, 0.999]
    weight_decay: 0.000125
    params: 
      -
        # DINOv3 params except norm/bias
        params: '^(?=.*.dinov3)(?!.*(?:norm|bn|bias)).*$'  
        lr: ${train.lrs.${model_name}.backbone_lr}
      -
        # DINOv3 norm/bias params
        params: '^(?=.*.dinov3)(?=.*(?:norm|bn|bias)).*$'    
        lr: ${train.lrs.${model_name}.backbone_lr}
        weight_decay: 0.
      - 
        # All other norm/bias params (sta, encoder, decoder, etc.)
        params: '^(?=.*(?:sta|encoder|decoder))(?=.*(?:norm|bn|bias)).*$'
        weight_decay: 0.
      -
        # All other parameters (e.g., non-norm/bias from encoder/decoder)
        params: '.*'

  ### Scheduler ###
  lr_scheduler:
    type: flatcosine # 'onecycle' or 'flatcosine'
    # For OneCycleLR
    cycler_pct_start: 0.1
    # For FlatCosineLRScheduler
    lr_gamma: 0.5
    warmup_iter: 2000
    flat_epochs: 29
    no_aug_epochs: 8  # epochs at the end without heavy augmentations

  label_smoothing: 0.0
  
  ### Augs ###
  # Augmentation policy based on DEIMv2
  # [start_strong_augs_epoch, start_flat_lr_epoch, stop_strong_augs_epoch]
  aug_policy:
    epoch: [4, 29, 50]
    ops: ['Mosaic', 'RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']

  mixup_augs:
    mixup_prob: 0.5 # Probability to apply mixup within the policy range
    mixup_epochs: [4, 29]

  copyblend_augs:
    copyblend_prob: 0.5
    copyblend_type: blend # 'blend' or 'paste'
    copyblend_epochs: [4, 50]
    area_threshold: 100
    num_objects: 3
    with_expand: True
    random_num_objects: True # if true, will use random number of objects from 1 to num_objects
    expand_ratios: [0.1, 0.25]

  mosaic_augs:
    mosaic_prob: 0.5 # Probability to apply mosaic within the policy range
    mosaic_scale: [0.5, 1.5]
    degrees: 0.0
    translate: 0.1
    shear: 0.0
  
  augs:
    multiscale_prob: 0.5
    photometric_distort_p: 0.5
    iou_crop_p: 0.8
    zoom_out_p: 0.5
    rotation_degree: 10
    rotation_p: 0.0
    rotate_90: 0.05
    left_right_flip: 0.5
    up_down_flip: 0.0
    to_gray: 0.01
    blur: 0.01
    gamma: 0.02
    brightness: 0.02
    noise: 0.01
    coarse_dropout: 0.0

  ### Reproducibility ###
  seed: 42
  cudnn_fixed: False

  ### Recommended learning rates ###
  lrs:
    n:
      backbone_lr: 0.0004
      base_lr: 0.0008
    s:
      backbone_lr: 0.00006
      base_lr: 0.00025
    m:
      backbone_lr: 0.00002
      base_lr: 0.00015
    l:
      backbone_lr: 0.0000125 # This is the specific LR for dinov3 parameters
      base_lr: 0.0005   # This is the LR for other parameters
    x:
      backbone_lr: 0.0000015
      base_lr: 0.0001

split:
  ignore_negatives: False # only use images with labels
  shuffle: True
  train_split: 0.85
  val_split: 0.15 # test_split = 1 - train_split - val_split


export:
  half: False
  max_batch_size: 1
  dynamic_input: False


infer:
  to_crop: True  # if True - saves crops of detected objects
  paddings: # if int - amount of pixes, if float - percentage of image size
    w: 0.05
    h: 0.05


### service ###
defaults:
  - _self_
#  - override hydra/hydra_logging: disabled
#  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

now_dir: &nowdir ${now:%Y-%m-%d}
